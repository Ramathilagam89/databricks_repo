{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Read & Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942b390d-072f-4bc5-88dd-79c2109648eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists landing_zone;\n",
    "create volume if not exists landing_vol;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a45f022a-8441-4e1a-9871-60ea17314e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# In the above cell,I didnt mentioned the full schema name(telecom_catalog_assign.landing_zone) and volume name(telecom_catalog_assign.landing_zone.landing_vol.So I dropped the schema and volume here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67946073-0837-42ae-af74-2913d9a48ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "\n",
    "\n",
    "drop schema if exists landing_zone;\n",
    "drop volume if exists landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1d417f-fd44-43cb-ba6f-06ebf64a1c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9102be-26e6-452e-9da2-c96257aff758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Created New directories\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fafc169-3619-4e2d-8823-de6137ca9f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_Volumes_** are the modern, governed way to store files in Databricks using Unity Catalog paths like /Volumes/catalog/schema/volume/file.csv—think secure folders with permissions and tracking.\n",
    "​\n",
    "\n",
    "**_DBFS/FileStore_** is the older filesystem (paths like dbfs:/FileStore/file.csv) for quick workspace files without much governance.\n",
    "​\n",
    "\n",
    "_Volumes_ provide centralized governance, security, and auditing for production files through Unity Catalog, preventing unauthorized access and tracking usage across teams.\n",
    "​\n",
    "\n",
    "Key Production Benefits\n",
    "Permissions Control: Fine-grained access (e.g., only data engineers read raw files, analysts read processed)—no one bypasses Unity Catalog rules.\n",
    "​\n",
    "\n",
    "Audit & Lineage: Tracks who accessed/modified files, integrates with table lineage for full pipeline visibility (critical for compliance like GDPR).\n",
    "​\n",
    "\n",
    "Sharing Across Workspaces: Securely share volumes between dev/test/prod environments or users without copying data.\n",
    "​\n",
    "\n",
    "Why Not DBFS/FileStore?\n",
    "DBFS lacks these controls—anyone with workspace access can read/write, no audit trail, and it's not scalable for teams or regulated industries.\n",
    "​\n",
    "\n",
    "Use Volumes in production to avoid \"wild west\" file access and ensure your data pipelines stay compliant and secure.\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write dbutils.fs code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85217c57-bc9c-487a-ade8-6c4f8a091dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing the files inside the Volumes\n",
    "customer_csv = ''' 101,Arun,31,Chennai,PREPAID 102,Meera,45,Bangalore,POSTPAID 103,Irfan,29,Hyderabad,PREPAID 104,Raj,52,Mumbai,POSTPAID 105,,27,Delhi,PREPAID 106,Sneha,abc,Pune,PREPAID '''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, True)\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count 101\\t320\\t1500\\t20 102\\t120\\t4000\\t5 103\\t540\\t600\\t52 104\\t45\\t200\\t2 105\\t0\\t0\\t0 '''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\", usage_tsv, True)\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp 5001|101|TWR01|-80|2025-01-10 10:21:54 5004|104|TWR05|-75|2025-01-10 11:01:12 '''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.txt\", tower_logs_region1, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaed7ef-161e-4ca5-a24c-3fa054d1bf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Listing the files\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\"))\n",
    "print(dbutils.fs.head(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\"))\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\"))\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Spark Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6568f239-69cd-4849-9479-80fd0d6d1933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading the files using Recursivefilelookup and pathglobalfilter\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "df1=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",pathGlobFilter=\"*\",recursiveFileLookup=True)\n",
    "#df2=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/\",pathGlobFilter=\"*\")\n",
    "print(dbutils.fs.head(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.txt\"))\n",
    "display(df1)\n",
    "#display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ceeccab-6255-4554-ab1c-16b13d3104cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading the files using Recursivefilelookup and pathglobalfilter\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "paths = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"\n",
    "]\n",
    "df2=spark.read.options(pathGlobFilter=\"*\",recursiveFileLookup=True).text(paths)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5584e377-0824-4722-8ad4-141df9b6f96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading file using the option & options format\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "df1=spark.read.option(\"header\",\"false\").option(\"inferSchema\",\"false\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "display(df1)\n",
    "df1.printSchema()\n",
    "df2=spark.read.options(header=\"false\",inferSchema=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "display(df2)\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4037845-5642-4f9b-a55b-a1b161c64c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfe512c-cea9-4287-8593-e9f5528f023e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**header=True**\n",
    "Takes the __first_ record_ as header<br>\n",
    "**inferSchema=True**\n",
    "It find the each columns datatypes appropriately.Generally CSV consider all the datas as a _\"String\"_ datatype<br>\n",
    "**Printschema**\n",
    "It is used to print the infered datatypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051d8f7e-5977-49de-b00a-dcfac3b82a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How schema inference handled “abc” in age?**\n",
    "It handles \"abc\" as a \"String\" datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7e9892-7869-4cdf-96ea-846fd2000580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Applying the toDF method and some formats like sep,lineSep\n",
    "df1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",sep=\",\",inferSchema=\"true\",header=\"true\",lineSep=\" \").toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "df1.show()\n",
    "df1.printSchema()#In the age column we have a data as abc,so it consider the datatype as String\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870cceb5-bb70-4e2a-9396-4c1c76660165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count 101\\t320\\t1500\\t20 102\\t120\\t4000\\t5 103\\t540\\t600\\t52 104\\t45\\t200\\t2 105\\t0\\t0\\t0 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff81635-328c-4928-97bf-67c7f24c1dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_val=\"customer_id int,voice_mins int,data_mb int,sms_count int\"\n",
    "df1_sam=spark.read.schema(schema_val).options(sep=\"\\t\",lineSep=\" \",header=\"true\").csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "df1_sam.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccae6136-7dab-4075-9686-a2571578fa73",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"timestamp\":356},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766354233085}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,TimestampType\n",
    "custom_schema=StructType([StructField(\"event_id\",IntegerType()),\n",
    "                          StructField(\"customer_id\",IntegerType()),\n",
    "                          StructField(\"tower_id\",StringType()),\n",
    "                          StructField(\"signal_strength\",IntegerType()),\n",
    "                          StructField(\"timestamp\",TimestampType())])\n",
    "df1=spark.read.format(\"csv\").options(sep=\"|\",linesep=\" \",header=\"true\").schema(custom_schema).load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.txt\")\n",
    "df1.display()\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b999877-6239-4893-a98c-dd25c5ce23eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above output(Timestamp),time is not displayed.Because of the linesep=\" \",It takes the space between the date,time and displays the time into newline.But the datatype of each column is not matched,it displays null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ce3a63-1a57-4f4e-88f9-b84f951f3a2d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766353091640}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I tried this program by asking databricks to infer the schema by what datatype it understood and how it displays\n",
    "df1 = spark.read.format(\"csv\").options(header=\"true\",sep=\"|\",inferSchema=\"true\",linesep=\" \").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.txt\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e34c3bc-962d-438d-a1b6-ac27d2da6608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb80e3b-7313-4060-96ac-f28036a19bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Spark Dataframes\").getOrCreate()\n",
    "customer_csv = ''' 101,Arun,31,Chennai,PREPAID 102,Meera,45,Bangalore,POSTPAID 103,Irfan,29,Hyderabad,PREPAID 104,Raj,52,Mumbai,POSTPAID 105,,27,Delhi,PREPAID 106,Sneha,abc,Pune,PREPAID '''\n",
    "dfnew=spark.createDataFrame(customer_csv)\n",
    "display(dfnew)\n",
    "dfnew.write.format(\"csv\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custnew.csv\",mode=\"overwrite\",sep=\",\",lineSep=\" \")\n",
    "display(dfnew)\n",
    "dfnew.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33baa76d-8105-4e18-8498-3d7fc6ac5e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfnew=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custnew.csv\")\n",
    "display(dfnew)\n",
    "dfnew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eaa8dd7-533e-42e6-b7b8-6bf823d9d219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\",header=\"false\",sep=\",\",lineSep=\" \",mode=\"overwrite\")\n",
    "df2=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\")\n",
    "display(df2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87ae3bf-18f4-447b-9aaf-978276195b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagenew.csv\",header=\"false\",sep=\",\",lineSep=\" \",mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3613e5-3b02-43d4-ae68-69e316e8315f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df1.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"True\",sep=\"|\",lineSep=\" \")\n",
    "df2=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"true\",sep=\"|\",lineSep=\" \")\n",
    "df2.show(2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34158cf6-dd7f-40d6-9969-ed76710540a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8f233d-5b34-42b9-975f-35f5f4f48f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\", lineSep=\" \", sep=\",\")\n",
    "display(df_cust)\n",
    "df_cust.write.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_json.json\", mode=\"overwrite\")\n",
    "df_cust_json=spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_json.json\") \n",
    "display(df_cust_json.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07528cc-088f-411a-8e62-37d172be87d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_usage=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=\"false\",sep=\"\\t\",lineSep=\" \")\n",
    "display(df_usage)\n",
    "df_usage.write.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_json.json\", mode=\"ignore\", compression=\"snappy\")\n",
    "df_usage_json=spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_json.json\") \n",
    "display(df_usage_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3784b2ad-5e26-4229-8086-262955491c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"true\",sep=\"|\",lineSep=\" \")\n",
    "display(df_tower)\n",
    "df_tower.write.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_json.json\", mode=\"ignore\")\n",
    "df_tower_json=spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_json.json\") \n",
    "display(df_tower_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f2ba69-3cde-4ec6-8945-e4ef9f7bb109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cea8c0d-b707-4eea-9c98-1506f0a87cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\", lineSep=\" \", sep=\",\")\n",
    "display(df_cust)\n",
    "df_cust.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_parquet\", mode=\"overwrite\", compression=\"gzip\")\n",
    "df_cust_parquet=spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_parquet\")\n",
    "display(df_cust_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210a5c52-0f01-4e14-a98f-861d138516f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_usage=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=\"true\",sep=\"\\t\",lineSep=\" \")\n",
    "display(df_usage)\n",
    "df_usage.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_parquet\", mode=\"error\")\n",
    "df_usage_parquet=spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_parquet\")\n",
    "display(df_usage_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b41c794f-5cfc-4aeb-a599-e6d4a47a0f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8387a70-e11e-480c-9af8-262461c1bd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"true\",sep=\"|\",lineSep=\" \")\n",
    "display(df_tower)\n",
    "df_tower.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_orc\", mode=\"append\")\n",
    "df_tower_orc=spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_orc\")\n",
    "display(df_tower_orc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35761315-0b0f-46ff-9c3d-c0405bce7b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad32e953-5f62-4773-bc2b-a666855df2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"true\",sep=\"|\",lineSep=\" \")\n",
    "display(df_tower)\n",
    "df_tower.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_delta\", mode=\"overwrite\")\n",
    "df_tower_delta=spark.read.load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_delta\")\n",
    "display(df_tower_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6dd0890-02bd-4acd-b837-daceb256c706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8faa8cd7-1ea3-44d4-b623-e18eac3d3cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\", lineSep=\" \", sep=\",\")\n",
    "display(df_cust)\n",
    "df_cust.write.saveAsTable(\"cust_table\", mode=\"overwrite\")\n",
    "df_cust_table=spark.read.table(\"cust_table\")\n",
    "display(df_cust_table)\n",
    "df1=spark.sql(\"\"\"select * from cust_table where _c3='Chennai'\"\"\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af91225-438e-45d0-bb50-bc5e83cd1fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_new_region.csv\",header=\"true\",sep=\"|\",lineSep=\" \")\n",
    "df_tower.write.insertInto(\"cust_table\")\n",
    "df_tower_delta=spark.read.table(\"cust_table\")\n",
    "display(df_tower_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984b7075-0272-4952-a697-0bce3d049a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists cust_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aac447b-690b-4562-99dd-0ce096e9ad55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14457aa9-d6a4-4be3-87c8-b46ca6584f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below code is to test the 1st usecase as mentioned above. We Can't insertinto a \"New\" table. it is only possible to insertinto an \"Existing\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed92efba-4e7d-4de8-9151-d122b00d2409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custwrt.csv\", lineSep=\" \", sep=\",\")\n",
    "display(df_cust)\n",
    "df_tower.write.insertInto(\"cust_table_new\")\n",
    "df_tower_delta=spark.read.table(\"cust_table_new\")\n",
    "display(df_tower_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c4bce3-4bd3-4db6-a074-02bb24c5f91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aaf9d1-0194-4671-bfdb-b01c3c32fb4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust_parquet=spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_parquet\")\n",
    "display(df_cust_parquet)\n",
    "df_cust_parquet.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_xml\",mode=\"append\", rowTag=\"customer\", rootTag=\"Cust_Check\")\n",
    "df_cust_xml=spark.read.format(\"xml\").option(\"rowTag\",\"customer\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cust_xml\")\n",
    "display(df_cust_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e2fe69-9352-4ec9-bf70-15d760c89aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ac5176-95b3-4838-a9c2-fb6316c59637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6e39ec-752d-4183-9656-2b6d7938922d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##16. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a862f995-6e02-49c8-8a96-e8965f96a621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**When to use/benefits csv**- when we have the data in structured format, we can use CSV.\n",
    "\n",
    "**When to use/benefits json/XML** - when we have the data in semi structured format (dictonary like Key:value pair), we can use json. It will be used between APIs as its commonly used format. XML is rarely used though.\n",
    "\n",
    "**When to use/benefit orc\n",
    "When to use/benefit parquet\n",
    "When to use/benefit delta**\n",
    "\n",
    "All the above formats shall be used to store big amount of data in efficient storage & secured way, process optimization in downstream systems. It will be used for interoperability (Between different platforms like Bigquery, Azure, etc..)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5054901096864729,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Usecase3_Practise:read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
